{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Model in Pandas\n",
    "\n",
    "A bag of words is a matrix representation of a document. It consists of several columns which are unique words. And every row is a new document. The cell values of every column indicate whether the word is present in the document or not. A dataframe representation is shown below. \n",
    "\n",
    "![Image](./data/bagofwords.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column (except doc_id) is a word. Each row is a new document. The first column is the name of the document. The first row is telling us that doc_id 1987_1 does not have the word abalone, abbeel or zhou. Hence each value is 0. If the word is contained in the document then that corresponding value in the column is 1.     \n",
    "\n",
    "We have to build this bag of words model with 5 documents.  The documents are named as doc1.txt, doc2.txt, doc3.txt, doc4.txt and doc5.txt.  \n",
    "\n",
    "\n",
    "**There should be 5 rows in the dataframe. The columns should be unique words in all documents. The columns should have words with length greater than 4. The words should not have any punctuation marks with it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['In',\n",
       "  'this',\n",
       "  'tutorial',\n",
       "  'competition,',\n",
       "  'we',\n",
       "  'dig',\n",
       "  'a',\n",
       "  'little',\n",
       "  '\"deeper\"',\n",
       "  'into',\n",
       "  'sentiment',\n",
       "  'analysis.',\n",
       "  \"Google's\",\n",
       "  'Word2Vec',\n",
       "  'is',\n",
       "  'a',\n",
       "  'deep-learning',\n",
       "  'inspired',\n",
       "  'method',\n",
       "  'that',\n",
       "  'focuses',\n",
       "  'on',\n",
       "  'the',\n",
       "  'meaning',\n",
       "  'of',\n",
       "  'words.',\n",
       "  'Word2Vec',\n",
       "  'attempts',\n",
       "  'to',\n",
       "  'understand',\n",
       "  'meaning',\n",
       "  'and',\n",
       "  'semantic',\n",
       "  'relationships',\n",
       "  'among',\n",
       "  'words.',\n",
       "  'It',\n",
       "  'works',\n",
       "  'in',\n",
       "  'a',\n",
       "  'way',\n",
       "  'that',\n",
       "  'is',\n",
       "  'similar',\n",
       "  'to',\n",
       "  'deep',\n",
       "  'approaches,',\n",
       "  'such',\n",
       "  'as',\n",
       "  'recurrent',\n",
       "  'neural',\n",
       "  'nets',\n",
       "  'or',\n",
       "  'deep',\n",
       "  'neural',\n",
       "  'nets,',\n",
       "  'but',\n",
       "  'is',\n",
       "  'computationally',\n",
       "  'more',\n",
       "  'efficient.',\n",
       "  'This',\n",
       "  'tutorial',\n",
       "  'focuses',\n",
       "  'on',\n",
       "  'Word2Vec',\n",
       "  'for',\n",
       "  'sentiment',\n",
       "  'analysis.'],\n",
       " ['Sentiment',\n",
       "  'analysis',\n",
       "  'is',\n",
       "  'a',\n",
       "  'challenging',\n",
       "  'subject',\n",
       "  'in',\n",
       "  'machine',\n",
       "  'learning.',\n",
       "  'People',\n",
       "  'express',\n",
       "  'their',\n",
       "  'emotions',\n",
       "  'in',\n",
       "  'language',\n",
       "  'that',\n",
       "  'is',\n",
       "  'often',\n",
       "  'obscured',\n",
       "  'by',\n",
       "  'sarcasm,',\n",
       "  'ambiguity,',\n",
       "  'and',\n",
       "  'plays',\n",
       "  'on',\n",
       "  'words,',\n",
       "  'all',\n",
       "  'of',\n",
       "  'which',\n",
       "  'could',\n",
       "  'be',\n",
       "  'very',\n",
       "  'misleading',\n",
       "  'for',\n",
       "  'both',\n",
       "  'humans',\n",
       "  'and',\n",
       "  'computers.',\n",
       "  \"There's\",\n",
       "  'another',\n",
       "  'Kaggle',\n",
       "  'competition',\n",
       "  'for',\n",
       "  'movie',\n",
       "  'review',\n",
       "  'sentiment',\n",
       "  'analysis.',\n",
       "  'In',\n",
       "  'this',\n",
       "  'tutorial',\n",
       "  'we',\n",
       "  'explore',\n",
       "  'how',\n",
       "  'Word2Vec',\n",
       "  'can',\n",
       "  'be',\n",
       "  'applied',\n",
       "  'to',\n",
       "  'a',\n",
       "  'similar',\n",
       "  'problem.'],\n",
       " ['Deep',\n",
       "  'learning',\n",
       "  'has',\n",
       "  'been',\n",
       "  'in',\n",
       "  'the',\n",
       "  'news',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'over',\n",
       "  'the',\n",
       "  'past',\n",
       "  'few',\n",
       "  'years,',\n",
       "  'even',\n",
       "  'making',\n",
       "  'it',\n",
       "  'to',\n",
       "  'the',\n",
       "  'front',\n",
       "  'page',\n",
       "  'of',\n",
       "  'the',\n",
       "  'New',\n",
       "  'York',\n",
       "  'Times.',\n",
       "  'These',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'techniques,',\n",
       "  'inspired',\n",
       "  'by',\n",
       "  'the',\n",
       "  'architecture',\n",
       "  'of',\n",
       "  'the',\n",
       "  'human',\n",
       "  'brain',\n",
       "  'and',\n",
       "  'made',\n",
       "  'possible',\n",
       "  'by',\n",
       "  'recent',\n",
       "  'advances',\n",
       "  'in',\n",
       "  'computing',\n",
       "  'power,',\n",
       "  'have',\n",
       "  'been',\n",
       "  'making',\n",
       "  'waves',\n",
       "  'via',\n",
       "  'breakthrough',\n",
       "  'results',\n",
       "  'in',\n",
       "  'image',\n",
       "  'recognition,',\n",
       "  'speech',\n",
       "  'processing,',\n",
       "  'and',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'tasks.',\n",
       "  'Recently,',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'approaches',\n",
       "  'won',\n",
       "  'several',\n",
       "  'Kaggle',\n",
       "  'competitions,',\n",
       "  'including',\n",
       "  'a',\n",
       "  'drug',\n",
       "  'discovery',\n",
       "  'task,',\n",
       "  'and',\n",
       "  'cat',\n",
       "  'and',\n",
       "  'dog',\n",
       "  'image',\n",
       "  'recognition.'],\n",
       " ['Since',\n",
       "  'deep',\n",
       "  'learning',\n",
       "  'is',\n",
       "  'a',\n",
       "  'rapidly',\n",
       "  'evolving',\n",
       "  'field,',\n",
       "  'large',\n",
       "  'amounts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'work',\n",
       "  'has',\n",
       "  'not',\n",
       "  'yet',\n",
       "  'been',\n",
       "  'published,',\n",
       "  'or',\n",
       "  'exists',\n",
       "  'only',\n",
       "  'as',\n",
       "  'academic',\n",
       "  'papers.',\n",
       "  'Part',\n",
       "  '3',\n",
       "  'of',\n",
       "  'the',\n",
       "  'tutorial',\n",
       "  'is',\n",
       "  'more',\n",
       "  'exploratory',\n",
       "  'than',\n",
       "  'prescriptive',\n",
       "  '--',\n",
       "  'we',\n",
       "  'experiment',\n",
       "  'with',\n",
       "  'several',\n",
       "  'ways',\n",
       "  'of',\n",
       "  'using',\n",
       "  'Word2Vec',\n",
       "  'rather',\n",
       "  'than',\n",
       "  'giving',\n",
       "  'you',\n",
       "  'a',\n",
       "  'recipe',\n",
       "  'for',\n",
       "  'using',\n",
       "  'the',\n",
       "  'output.',\n",
       "  '',\n",
       "  'To',\n",
       "  'achieve',\n",
       "  'these',\n",
       "  'goals,',\n",
       "  'we',\n",
       "  'rely',\n",
       "  'on',\n",
       "  'an',\n",
       "  'IMDB',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  'data',\n",
       "  'set,',\n",
       "  'which',\n",
       "  'has',\n",
       "  '',\n",
       "  'multi-paragraph',\n",
       "  'movie',\n",
       "  'reviews,',\n",
       "  'both',\n",
       "  'positive',\n",
       "  'and',\n",
       "  'negative.'],\n",
       " ['Word2Vec',\n",
       "  'does',\n",
       "  'not',\n",
       "  'need',\n",
       "  'labels',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'create',\n",
       "  'meaningful',\n",
       "  'representations.',\n",
       "  'This',\n",
       "  'is',\n",
       "  'useful,',\n",
       "  'since',\n",
       "  'most',\n",
       "  'data',\n",
       "  'in',\n",
       "  'the',\n",
       "  'real',\n",
       "  'world',\n",
       "  'is',\n",
       "  'unlabeled.',\n",
       "  'If',\n",
       "  'the',\n",
       "  'network',\n",
       "  'is',\n",
       "  'given',\n",
       "  'enough',\n",
       "  'training',\n",
       "  'data',\n",
       "  '(tens',\n",
       "  'of',\n",
       "  'billions',\n",
       "  'of',\n",
       "  'words),',\n",
       "  'it',\n",
       "  'produces',\n",
       "  'word',\n",
       "  'vectors',\n",
       "  'with',\n",
       "  'intriguing',\n",
       "  'characteristics.',\n",
       "  'Words',\n",
       "  'with',\n",
       "  'similar',\n",
       "  'meanings',\n",
       "  'appear',\n",
       "  'in',\n",
       "  'clusters,',\n",
       "  'and',\n",
       "  'clusters',\n",
       "  'are',\n",
       "  'spaced',\n",
       "  'such',\n",
       "  'that',\n",
       "  'some',\n",
       "  'word',\n",
       "  'relationships,',\n",
       "  'such',\n",
       "  'as',\n",
       "  'analogies,',\n",
       "  'can',\n",
       "  'be',\n",
       "  'reproduced',\n",
       "  'using',\n",
       "  'vector',\n",
       "  'math.']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load file pointers with file locations\n",
    "fp1 = open('data/Bag of Words Docs/doc1.txt','r')\n",
    "fp2 = open('data/Bag of Words Docs/doc2.txt','r')\n",
    "fp3 = open('data/Bag of Words Docs/doc3.txt','r')\n",
    "fp4 = open('data/Bag of Words Docs/doc4.txt','r')\n",
    "fp5 = open('data/Bag of Words Docs/doc5.txt','r')\n",
    "#characters to be stripped\n",
    "strips=',.-\"()'\n",
    "#List containing words from each doc\n",
    "doc=[]\n",
    "doc.append(fp1.read().split(' '))\n",
    "doc.append(fp2.read().split(' '))\n",
    "doc.append(fp3.read().split(' '))\n",
    "doc.append(fp4.read().replace('\\n',' ').split(' '))\n",
    "doc.append(fp5.read().split(' '))\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "51\n",
      "105\n",
      "168\n",
      "233\n",
      "290\n",
      "{'the', 'on', 'more', 'In', 'in', 'but', 'competition', 'tutorial', 'words', 'dig', 'little', 'neural', 'to', 'It', 'works', 'deep', 'recurrent', 'of', 'is', 'a', 'deeper', 'approaches', 'we', 'analysis', 'semantic', 'understand', 'efficient', 'This', 'method', 'for', 'focuses', 'meaning', 'similar', 'attempts', 'deep-learning', 'that', 'such', 'Word2Vec', 'this', 'sentiment', 'into', \"Google's\", 'as', 'relationships', 'or', 'computationally', 'among', 'inspired', 'nets', 'way', 'and'}\n"
     ]
    }
   ],
   "source": [
    "#List of sets having unique words in a doc\n",
    "newdoc=[]\n",
    "#newdoc contains non-duplicated words and stripped down symbolic characters\n",
    "for d in doc:\n",
    "    newdoc.append(set(map(lambda x:x.strip(strips),d)))\n",
    "l=0\n",
    "#Print number of unique words in each doc\n",
    "print(len(newdoc[0]))\n",
    "for d in newdoc:\n",
    "    l+=len(d)\n",
    "    print(l)\n",
    "print(newdoc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print set of unique words with length>4 in all docs\n",
    "docsset={*()}\n",
    "for d in newdoc:\n",
    "    for w in d:\n",
    "        if len(w)>4:\n",
    "            docsset.add(w)\n",
    "len(docsset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make rows for each doc\n",
    "Rows=[]\n",
    "for  i in range(4):\n",
    "    Rows.append([1 if col in newdoc[i] else 0 for col in docsset])\n",
    "Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>learning</th>\n",
       "      <th>subject</th>\n",
       "      <th>exists</th>\n",
       "      <th>prescriptive</th>\n",
       "      <th>papers</th>\n",
       "      <th>making</th>\n",
       "      <th>Words</th>\n",
       "      <th>computers</th>\n",
       "      <th>world</th>\n",
       "      <th>...</th>\n",
       "      <th>method</th>\n",
       "      <th>focuses</th>\n",
       "      <th>deep-learning</th>\n",
       "      <th>analogies</th>\n",
       "      <th>Word2Vec</th>\n",
       "      <th>speech</th>\n",
       "      <th>published</th>\n",
       "      <th>reproduced</th>\n",
       "      <th>these</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc2.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc3.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc4.txt</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc5.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id  learning  subject  exists  prescriptive  papers  making  Words  \\\n",
       "0  doc1.txt         0        0       0             0       0       0      0   \n",
       "1  doc2.txt         1        1       0             0       0       0      0   \n",
       "2  doc3.txt         1        0       0             0       0       1      0   \n",
       "3  doc4.txt         1        0       1             1       1       0      0   \n",
       "4  doc5.txt         0        0       0             0       0       0      1   \n",
       "\n",
       "   computers  world   ...    method  focuses  deep-learning  analogies  \\\n",
       "0          0      0   ...         1        1              1          0   \n",
       "1          1      0   ...         0        0              0          0   \n",
       "2          0      0   ...         0        0              0          0   \n",
       "3          0      0   ...         0        0              0          0   \n",
       "4          0      1   ...         0        0              0          1   \n",
       "\n",
       "   Word2Vec  speech  published  reproduced  these  output  \n",
       "0         1       0          0           0      0       0  \n",
       "1         1       0          0           0      0       0  \n",
       "2         0       1          0           0      0       0  \n",
       "3         1       0          1           0      1       1  \n",
       "4         1       0          0           1      0       0  \n",
       "\n",
       "[5 rows x 134 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create dataframe with unique words from all docs as columns and boolean values indicating presence of words in doc\n",
    "df=pd.DataFrame(Rows,columns=docsset)\n",
    "df.insert(loc=0, column='doc_id',value=['doc1.txt','doc2.txt','doc3.txt','doc4.txt','doc5.txt'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "Name: Word2Vec, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Print word appearing in 4 of the documents\n",
    "for j in df:\n",
    "    if df[j].sum()==4:\n",
    "        print(df[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
